{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Input a set of training examples**\n",
    "\n",
    "**2. For each training example $x$:** Set the correspoinding input activation $x^{x,1}$, and perform the following steps:\n",
    "-   **Feedforward:** For each $l = 2,3, ..., L$ compute $z^{x,l} = w^{l}a^{x,l-1} + b^{l}$ and $a^{x,l} = \\sigma(z^{x,l})$\n",
    "\n",
    "-   **Output error $\\delta^{x,L}$:** Compute the vector $\\delta^{x,L} = \\nabla_a C_x \\odot \\sigma'(z^{x,L})$\n",
    "\n",
    "-   **Backpropagate the error**: For each $l = L -1, L-2, ..., 2$ compute $\\delta^{x,l} = ((w^{l+1})^T \\delta^{x,l+1}) \\odot \\sigma'(z^{x,l})$\n",
    "\n",
    "**3.** $\\text{Gradient descent}:$ $\\text{For each}$ $l = L, L-1, ..., 2$  $\\text{update the weights according to the rule}$ \n",
    "$w^l \\rightarrow\n",
    "w^l-\\frac{\\eta}{m} \\sum_x \\delta^{x,l} (a^{x,l-1})^T,$\n",
    "$\\text{and the biases according to the rule}$ $b^l \\rightarrow\n",
    "b^l-\\frac{\\eta}{m} \\sum_x \\delta^{x,l}.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo:\n",
    "#\n",
    "# load data\n",
    "# data preprocessing\n",
    "# ----------------------------------------------------------------\n",
    "# Create a class for the neural network\n",
    "# 1. Input layer 2. Hidden layer 3. Output layer\n",
    "# ----------------------------------------------------------------\n",
    "# Feedforward Nueral Network:\n",
    "# ReLU|Sigmoid(np.dot(weight, activation) + bias)\n",
    "# ----------------------------------------------------------------\n",
    "# Stoachastic Gradient Descent (SGD):\n",
    "# Train the neural network using mini-batch stochastic gradient descent\n",
    "# ----------------------------------------------------------------\n",
    "# Update Mini Batch:\n",
    "# for each mini-batch, update the weights and biases using the gradient\n",
    "#   nabla_b = nabla_b + delta_nabla_b\n",
    "#   nabla_w = nabla_w + delta_nabla_w\n",
    "# weight = weight - (eta/len(mini_batch_size)) * nabla_w\n",
    "# bias = bias - (eta/len(mini_batch_size)) * nabla_b\n",
    "# ----------------------------------------------------------------\n",
    "# Backpropagation:\n",
    "# for each bias and weight\n",
    "#   z_vector = np.dot(weight, activation) + bias\n",
    "#   appply activation function sigmoid, ReLU, tanh, etc.\n",
    "# delta = ∇cost(z_vector, y) * ∇activation(z_vector) \n",
    "# nabla_b = delta\n",
    "# nabla_w = < delta, activation>\n",
    "# for each layer (input, hidden, output)\n",
    "#   update ∇bias and ∇weight\n",
    "# return ∇b, ∇w\n",
    "# ----------------------------------------------------------------\n",
    "# Define different activation functions\n",
    "# Sigmoid, ReLU, Softmax, tanh. \n",
    "# sigmoid(z) = 1 / (1 + exp(-z))\n",
    "# ReLU(z) = max(0, z)\n",
    "# Softmax(z) = exp(z) / sum(exp(z))\n",
    "# tanh(z) = (exp(z) - exp(-z)) / (exp(z) + exp(-z))\n",
    "# ----------------------------------------------------------------\n",
    "# define different layer classes\n",
    "# Fully connected layer (intermediate layer), Convolutional_Pooling_layer (), Softmax layer (output layer) \n",
    "# ----------------------------------------------------------------\n",
    "# Integrate numba {+ CuPy:{NumPy & SciPy for GPU}} for faster computation\n",
    "'''\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "cuda.detect()\n",
    "\n",
    "array = np.random.randint(0,255, size=(4000,4000))\n",
    "# array_gpu = cp.asarray(array)\n",
    "\n",
    "# call timeit inline function\n",
    "# %timeit cp.asarray(array)\n",
    "%timeit np.asarray(array)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "74e8ec9c84b340f4de24eed72001e08faf7103697f715d0f05647d14de0826d0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('RT-Academy': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
